{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb60257-b492-4ab4-bc6a-f72adba0d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sidda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sidda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features:\n",
      "['aahc' 'ability' 'able' 'academic' 'acceleration' 'acceptable' 'access'\n",
      " 'accessibility' 'according' 'account']\n"
     ]
    }
   ],
   "source": [
    "#practical 1\n",
    "\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required resources for nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to fetch and clean raw HTML from a URL\n",
    "def fetch_and_clean_html(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "\n",
    "    # Get text and remove extra whitespace\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Function to tokenize and remove stopwords\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Function for feature extraction using TF-IDF\n",
    "def extract_features(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://en.wikipedia.org/wiki/Web_mining'  # Example web page\n",
    "    raw_text = fetch_and_clean_html(url)\n",
    "    normalized_text = normalize_text(raw_text)\n",
    "    tokens = tokenize_and_remove_stopwords(normalized_text)\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    tfidf_matrix, feature_names = extract_features([processed_text])\n",
    "\n",
    "    print(\"Top 10 Features:\")\n",
    "    print(feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d591c29-5ae4-43a9-bb58-73a3111db56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Ratings Matrix (with NaNs):\n",
      "       Item1  Item2  Item3  Item4\n",
      "User1    5.0    3.0    4.0    NaN\n",
      "User2    4.0    NaN    2.0    3.0\n",
      "User3    NaN    NaN    5.0    4.0\n",
      "User4    1.0    1.0    NaN    2.0\n",
      "\n",
      "User-User Similarity Matrix:\n",
      "       User1  User2  User3  User4\n",
      "User1   1.00   0.98   0.98   0.94\n",
      "User2   0.98   1.00   0.96   0.91\n",
      "User3   0.98   0.96   1.00   0.94\n",
      "User4   0.94   0.91   0.94   1.00\n",
      "\n",
      "Predicted Ratings Matrix (NaNs filled):\n",
      "       Item1  Item2  Item3  Item4\n",
      "User1   5.00   3.00   4.00   3.01\n",
      "User2   4.00   2.04   2.00   3.00\n",
      "User3   3.36   2.02   5.00   4.00\n",
      "User4   1.00   1.00   3.68   2.00\n"
     ]
    }
   ],
   "source": [
    "#Pract 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample user-item rating matrix (rows: users, columns: items)\n",
    "data = {\n",
    "    'Item1': [5, 4, np.nan, 1],\n",
    "    'Item2': [3, np.nan, np.nan, 1],\n",
    "    'Item3': [4, 2, 5, np.nan],\n",
    "    'Item4': [np.nan, 3, 4, 2]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=['User1', 'User2', 'User3', 'User4'])\n",
    "\n",
    "print(\"Original Ratings Matrix (with NaNs):\")\n",
    "print(df)\n",
    "\n",
    "# Step 1: Fill missing values with user mean (mean imputation)\n",
    "df_filled = df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "\n",
    "# Step 2: Compute user-user cosine similarity\n",
    "similarity = cosine_similarity(df_filled)\n",
    "similarity_df = pd.DataFrame(similarity, index=df.index, columns=df.index)\n",
    "\n",
    "print(\"\\nUser-User Similarity Matrix:\")\n",
    "print(similarity_df.round(2))\n",
    "\n",
    "# Step 3: Predict missing ratings using weighted average of neighbors\n",
    "def predict_rating(user, item):\n",
    "    if not np.isnan(df.loc[user, item]):\n",
    "        return df.loc[user, item]  # return actual rating if it exists\n",
    "\n",
    "    # Users who have rated this item\n",
    "    users_who_rated = df[item].dropna().index\n",
    "    sims = similarity_df.loc[user, users_who_rated]\n",
    "    ratings = df.loc[users_who_rated, item]\n",
    "\n",
    "    if sims.sum() == 0:\n",
    "        return df.loc[user].mean()  # fallback: user's average\n",
    "\n",
    "    # Weighted average\n",
    "    weighted_sum = np.dot(sims, ratings)\n",
    "    return weighted_sum / sims.sum()\n",
    "\n",
    "# Step 4: Create a predicted rating matrix\n",
    "predicted_df = df.copy()\n",
    "for user in df.index:\n",
    "    for item in df.columns:\n",
    "        predicted_df.loc[user, item] = predict_rating(user, item)\n",
    "\n",
    "print(\"\\nPredicted Ratings Matrix (NaNs filled):\")\n",
    "print(predicted_df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d378c08-37fd-42e7-ad28-abc98d4b5715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Crawling the web...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pract 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import heapq\n",
    "\n",
    "# Basic crawler\n",
    "def crawl(start_url, depth=1):\n",
    "    visited = set()\n",
    "    queue = [(start_url, 0)]\n",
    "    documents = {}\n",
    "\n",
    "    while queue:\n",
    "        url, cur_depth = queue.pop(0)\n",
    "        if url in visited or cur_depth > depth:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            documents[url] = clean_text(text)\n",
    "            visited.add(url)\n",
    "\n",
    "            if cur_depth < depth:\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    abs_url = urljoin(url, link['href'])\n",
    "                    if urlparse(abs_url).scheme in ['http', 'https']:\n",
    "                        queue.append((abs_url, cur_depth + 1))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Clean and tokenize text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build inverted index\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(list)\n",
    "    for url, words in documents.items():\n",
    "        word_counts = Counter(words)\n",
    "        for word, count in word_counts.items():\n",
    "            index[word].append((url, count))\n",
    "    return index\n",
    "\n",
    "# Search function (ranking by frequency)\n",
    "def search(query, index):\n",
    "    query = query.lower().split()\n",
    "    scores = defaultdict(int)\n",
    "\n",
    "    for word in query:\n",
    "        for url, freq in index.get(word, []):\n",
    "            scores[url] += freq\n",
    "\n",
    "    ranked = heapq.nlargest(5, scores.items(), key=lambda x: x[1])\n",
    "    return ranked\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://en.wikipedia.org/wiki/Web_search_engine\"\n",
    "    print(\"ðŸ” Crawling the web...\")\n",
    "    docs = crawl(start_url, depth=1)\n",
    "\n",
    "    print(\"\\nâš™ï¸ Building index...\")\n",
    "    index = build_inverted_index(docs)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nEnter search query (or type 'exit'): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        results = search(query, index)\n",
    "        if results:\n",
    "            print(\"\\nTop results:\")\n",
    "            for url, score in results:\n",
    "                print(f\"{url} (score: {score})\")\n",
    "        else:\n",
    "            print(\"No results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de8e476-2f0b-412c-af20-f38d425008e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š PageRank for Example Graph (with circular & dangling nodes):\n",
      "A: 0.3175\n",
      "B: 0.3175\n",
      "C: 0.3175\n",
      "D: 0.0476\n",
      "\n",
      "ðŸ“Š PageRank for Empty Graph:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "#pract 4\n",
    "import networkx as nx\n",
    "\n",
    "def pagerank(graph, damping=0.85, max_iter=100, tol=1.0e-6):\n",
    "    if len(graph) == 0:\n",
    "        return {}\n",
    "\n",
    "    nodes = list(graph.keys())\n",
    "    N = len(nodes)\n",
    "    ranks = {node: 1.0 / N for node in nodes}\n",
    "\n",
    "    # Build adjacency list\n",
    "    outlinks = {node: set(graph[node]) for node in nodes}\n",
    "    for node in nodes:\n",
    "        outlinks[node] = set(outlinks[node]) & set(nodes)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        new_ranks = {}\n",
    "        for node in nodes:\n",
    "            rank_sum = 0.0\n",
    "            for src in nodes:\n",
    "                if node in outlinks[src]:\n",
    "                    rank_sum += ranks[src] / len(outlinks[src]) if len(outlinks[src]) > 0 else 0\n",
    "                elif len(outlinks[src]) == 0:\n",
    "                    rank_sum += ranks[src] / N  # Dangling node contribution\n",
    "\n",
    "            new_ranks[node] = (1 - damping) / N + damping * rank_sum\n",
    "\n",
    "        # Check for convergence\n",
    "        diff = sum(abs(new_ranks[node] - ranks[node]) for node in nodes)\n",
    "        ranks = new_ranks\n",
    "        if diff < tol:\n",
    "            break\n",
    "\n",
    "    return ranks\n",
    "\n",
    "# Example Graphs\n",
    "example_graph = {\n",
    "    'A': ['B'],\n",
    "    'B': ['C'],\n",
    "    'C': ['A'],\n",
    "    'D': []  # Dangling node\n",
    "}\n",
    "\n",
    "empty_graph = {}\n",
    "\n",
    "# Run PageRank\n",
    "print(\"ðŸ“Š PageRank for Example Graph (with circular & dangling nodes):\")\n",
    "ranks = pagerank(example_graph)\n",
    "for node, score in ranks.items():\n",
    "    print(f\"{node}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š PageRank for Empty Graph:\")\n",
    "print(pagerank(empty_graph))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db80e0-df61-4cdf-814f-5255171a4e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
