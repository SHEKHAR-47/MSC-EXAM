{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fa5c295-2ef8-4c06-96d6-6d1b0e2613aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Love Coding..\n",
      "Geeks for Geeks helped me in this regard very much. \n",
      "\n",
      "I Love Geeks for Geeks..\n",
      "['I', 'Love', 'Coding', '..']\n",
      "['Geeks', 'for', 'Geeks', 'helped', 'me', 'in', 'this', 'regard', 'very', 'much', '.', '\\n']\n",
      "['I', 'Love', 'Geeks', 'for', 'Geeks', '..']\n"
     ]
    }
   ],
   "source": [
    "# 1: Segmentation & Tokenization\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# $ python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "document = '''I Love Coding.. Geeks for Geeks helped me in this regard very much. \n",
    "I Love Geeks for Geeks..'''\n",
    "\n",
    "# Segmentation\n",
    "doc = nlp(document)\n",
    "for i in doc.sents:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "doc = nlp(document)\n",
    "for sentence in doc.sents:\n",
    "    words = [word.text for word in sentence]\n",
    "    print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e77feb6-9737-4756-8109-a2326e875a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'cri', 'cri']\n",
      "['study', 'studying', 'cry', 'cry']\n"
     ]
    }
   ],
   "source": [
    "# 2: Stemming & Lemmatization\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "words = ['eating', 'eats', 'eaten', 'eat' ]\n",
    "words = ['studies', 'studying', 'cries', 'cry']\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(stemmed_words)\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29e563da-247e-4198-a83f-37f408dd618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'flame', 'that', 'burns', 'Twice', 'as', 'bright', 'burns', 'half', 'as', 'long']\n",
      "('The', 'flame', 'that')\n",
      "('flame', 'that', 'burns')\n",
      "('that', 'burns', 'Twice')\n",
      "('burns', 'Twice', 'as')\n",
      "('Twice', 'as', 'bright')\n",
      "('as', 'bright', 'burns')\n",
      "('bright', 'burns', 'half')\n",
      "('burns', 'half', 'as')\n",
      "('half', 'as', 'long')\n"
     ]
    }
   ],
   "source": [
    "# 3. NGram\n",
    "import nltk\n",
    "from nltk.util import ngrams, trigrams\n",
    "text = \"The flame that burns Twice as bright burns half as long\"\n",
    "words = nltk.word_tokenize(text)\n",
    "trigrams = ngrams(words, 3)\n",
    "for trigram in trigrams:\n",
    "    print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b53b4df4-b42d-4d4f-861b-0e52bc5f6a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoS tagging using HMM: [('Joe', 'NNP'), ('waited', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('train', 'NN'), (',', ','), ('but', 'CC'), ('the', 'DT'), ('train', 'NN'), ('was', 'VBD'), ('late', 'JJ')]\n",
      "PoS tagging using NN: [('Joe', 'NOUN'), ('waited', 'VERB'), ('for', 'ADP'), ('the', 'DET'), ('train', 'NOUN'), (',', '.'), ('but', 'CONJ'), ('the', 'DET'), ('train', 'NOUN'), ('was', 'VERB'), ('late', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "# 4. POS Tagging\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')\n",
    "text = \"Joe waited for the train, but the train was late\"\n",
    "words = nltk.word_tokenize(text)\n",
    "hmm_tagged = nltk.pos_tag(words)\n",
    "nn_tagged = nltk.pos_tag(words, tagset='universal')\n",
    "print(\"PoS tagging using HMM:\", hmm_tagged)\n",
    "print(\"PoS tagging using NN:\", nn_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac0b94f-d6de-4f4e-9fc6-03d29a93479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic tree: (S I/PRP ate/VBP hot/JJ ice-cream/NN ,/, before/IN match/JJ start/NN)\n"
     ]
    }
   ],
   "source": [
    "# 5. Syntactic Parsing\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('treebank')\n",
    "text = \"I ate hot ice-cream ,before match start\"\n",
    "words = nltk.word_tokenize(text)\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "syntactic_tree = nltk.ne_chunk(tagged_words, binary=True)\n",
    "print(\"Syntactic tree:\", syntactic_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f6298cc-0575-4f29-b621-4e458f7c6648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John nsubj likes VERB []\n",
      "likes ROOT likes VERB [John, Mary, is, .]\n",
      "Mary dobj likes VERB []\n",
      "because mark is AUX []\n",
      "she nsubj is AUX []\n",
      "is advcl likes VERB [because, she, beautiful]\n",
      "beautiful acomp is AUX []\n",
      ". punct likes VERB []\n"
     ]
    }
   ],
   "source": [
    "# 6. Dependency Parsing\n",
    "import spacy\n",
    "# $ python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"John likes Mary because she is beautiful.\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f777ef1-6199-47fc-8755-19ac4ce16ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Nitin\n",
      "ORG Indian Institute of technology\n",
      "GPE Bombay\n"
     ]
    }
   ],
   "source": [
    "# 7. Named Entity Recognition\n",
    "import spacy\n",
    "# $ python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Nitin is studying at Indian Institute of technology Bombay.\"\n",
    "doc = nlp(text)\n",
    "for entity in doc.ents:\n",
    "    print(entity.label_, entity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c7bd15c-fda7-4f18-af37-3d1aee57f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP\n",
      "involves a range of\n",
      "techniques, including tokenization, part-of-speech tagging, named entity\n",
      "recognition, and\n",
      "sentiment analysis, among others. NLP is used in a wide range of applications, from virtual assistants like Siri and\n",
      "Alexa to\n",
      "sentiment analysis, machine translation, and even content generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/me/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# 8. Text Summarization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from heapq import nlargest\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a branch of artificial intelligence that\n",
    "focuses on the\n",
    "interaction between computers and human language. NLP has been around for\n",
    "several decades, but\n",
    "recent advances in machine learning and deep learning have dramatically\n",
    "improved its capabilities.\n",
    "NLP is used in a wide range of applications, from virtual assistants like Siri and\n",
    "Alexa to\n",
    "sentiment analysis, machine translation, and even content generation. NLP\n",
    "involves a range of\n",
    "techniques, including tokenization, part-of-speech tagging, named entity\n",
    "recognition, and\n",
    "sentiment analysis, among others. These techniques can be used to analyze and\n",
    "understand human\n",
    "language in a variety of contexts, from social media posts to scientific literature.\n",
    "Despite its many\n",
    "successes, NLP remains a challenging field, as natural language is complex and\n",
    "often ambiguous.\n",
    "As NLP continues to evolve, it has the potential to transform the way we\n",
    "interact with technology\n",
    "and with each other, opening up new possibilities for communication,\n",
    "collaboration, and creativity.\n",
    "\"\"\"\n",
    "\n",
    "num_sentences = 2\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text.lower())  # Convert all words to lowercase\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_freq = {}\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "max_freq = max(word_freq.values())\n",
    "        \n",
    "for word in word_freq.keys():\n",
    "    word_freq[word] = (word_freq[word] / max_freq)\n",
    "\n",
    "sent_scores = {}\n",
    "for sentence in sentences:\n",
    "    for word in word_tokenize(sentence.lower()):\n",
    "        if word in word_freq.keys():\n",
    "            if len(sentence.split()) < 30:  # Use `split()` instead of `split(' ')`\n",
    "                if sentence not in sent_scores.keys():\n",
    "                    sent_scores[sentence] = word_freq[word]\n",
    "                else:\n",
    "                    sent_scores[sentence] += word_freq[word]\n",
    "                    \n",
    "summary_sentences = nlargest(num_sentences, sent_scores, key=sent_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21513f88-af26-40a3-9cf7-0a795796a985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
